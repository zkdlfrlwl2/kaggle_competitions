{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import Callback\n\nimport scipy.signal as signal\nfrom scipy.fft import fft, fftfreq\nfrom scipy.signal import blackman\nfrom scipy.signal import hilbert, chirp\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nset_seed=2021\nos.environ['PYTHONHASHSEED'] = str(set_seed)\nrandom.seed(set_seed)\nnp.random.seed(set_seed)\ntf.random.set_seed(set_seed)\n\nONE_FOLD_PLOT=True\nONE_FOLD_ONLY=True\nCOMPUTE_LSTM_IMPORTANCE=False\n\nprint(tf.__version__)","metadata":{"_uuid":"e331dbcc-0346-4019-9ff6-b890154a878b","_cell_guid":"3e5a0bd1-3e22-4b2c-a565-68985e55f95e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-10T02:22:04.537917Z","iopub.execute_input":"2021-11-10T02:22:04.538303Z","iopub.status.idle":"2021-11-10T02:22:09.764966Z","shell.execute_reply.started":"2021-11-10T02:22:04.538247Z","shell.execute_reply":"2021-11-10T02:22:09.763852Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nall_pressure = np.sort(train.pressure.unique())\nPRESSURE_MIN = all_pressure[0]\nPRESSURE_MAX = all_pressure[-1]\nPRESSURE_STEP = (all_pressure[1] - all_pressure[0])","metadata":{"_uuid":"ca71d87d-6594-4f31-906d-0b53cd4c1374","_cell_guid":"89eb257e-0ed2-4f8b-94d6-462a5e995eb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-10T02:22:09.771054Z","iopub.execute_input":"2021-11-10T02:22:09.771275Z","iopub.status.idle":"2021-11-10T02:22:23.216313Z","shell.execute_reply.started":"2021-11-10T02:22:09.771249Z","shell.execute_reply":"2021-11-10T02:22:23.215078Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## FE\n1. https://www.kaggle.com/marutama/eda-about-time-step-and-u-out/notebook","metadata":{}},{"cell_type":"code","source":"broken_one_list = [3178, 16315, 18117, 24127, 28942, 39045, 46324, 54129, 55244, 72104, 76037, 87776, 104001, 119689, 120878]\nbroken_two_list = [36175, 38415, 44245, 55851, 74766, 109693, 111439]\nu_out1_outlier = [44245, 118114, 120878]\nu_in_last_outlier = [54086]\n\ntotal_outlier_list = np.concatenate((np.array(broken_one_list), \n                                     np.array(broken_two_list), \n                                     np.array(u_out1_outlier), \n                                     np.array(u_in_last_outlier)), \n                                     axis=0)\nprint(len(total_outlier_list))\n# 중복값 제거\ntotal_outlier_list = set(total_outlier_list)\ntotal_outlier_list = list(total_outlier_list) \nprint(len(total_outlier_list))\nprint(len(total_outlier_list)*80)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:22:23.217454Z","iopub.execute_input":"2021-11-10T02:22:23.217676Z","iopub.status.idle":"2021-11-10T02:22:23.226916Z","shell.execute_reply.started":"2021-11-10T02:22:23.217650Z","shell.execute_reply":"2021-11-10T02:22:23.226096Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('original train shape: ', train.shape)\n# outlier 제거\nfor outlier_list in total_outlier_list:\n    # print(outlier_list)\n    train.drop(index=train[train['breath_id']==outlier_list].index, inplace=True)\n    \nprint('preprocessed train shape: ', train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:22:23.228953Z","iopub.execute_input":"2021-11-10T02:22:23.229215Z","iopub.status.idle":"2021-11-10T02:22:43.673361Z","shell.execute_reply.started":"2021-11-10T02:22:23.229187Z","shell.execute_reply":"2021-11-10T02:22:43.672447Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef add_features(df):\n    \n    df['time_delta'] = (df['time_step'].diff())#.astype('float32')\n    df['time_delta'].fillna(0, inplace=True)\n    df['time_delta'].mask(df['time_delta'] < 0, 0, inplace=True) \n    df['delta'] = (df['time_delta'] * df['u_in'])#.astype('float32')\n    df['area'] = (df.groupby('breath_id')['delta'].cumsum())#.astype('float32')\n\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()#.astype('float32')\n    \n    #new ---\n    #df['time_gap_shift2'] = (df['time_step'] - df['time_step'].shift(2).fillna(0))#.astype('float32')\n    #df['u_in_gap_shift2'] = (df['u_in'] - df['u_in'].shift(2).fillna(0))#.astype('float32')\n    #df['u_in_rate_shift2'] = (df['u_in_gap_shift2'] / df['time_gap_shift2'])#.astype('float32')\n    #df = df.replace([np.inf, -np.inf], 0)\n    # ---\n    print('1 step compelte')\n    \n    #new ---\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] = (df['u_in_cumsum'] / df['count'])#.astype('float32')\n    df = df.drop(['count','one'], axis=1)\n    print('2 step compelte')\n    # ---\n \n    # new ---\n    df['u_in_diff'] = ((df['u_in']).groupby(df['breath_id']).diff())#.astype('float32')\n    df['u_in_diff'].fillna(0, inplace=True) \n    \n    df['diff_sign'] = np.sign(df['u_in_diff'])\n    df['sign_diff'] = (df['diff_sign']).groupby(df['breath_id']).diff()\n    df['sign_diff'].fillna(0, inplace=True)\n    df['sign_diff'] = abs(df['sign_diff']) \n    \n    sign_diff_dict = df.groupby('breath_id')['sign_diff'].sum().to_dict()\n    df['diff_vib'] = df['breath_id'].map(sign_diff_dict) \n    df['diff_vib'] = df['diff_vib']#.astype('float32')\n    \n    if 'diff_sign' in df.columns:\n        df.drop(['diff_sign', 'sign_diff'], axis=1, inplace=True)\n    print('3 step compelte')\n    #---\n    \n    # new ---\n    df['u_in_cumsum_u_out_0'] = df[df['u_out']==0]['u_in'].groupby(df['breath_id']).cumsum()\n    df['u_in_cumsum_u_out_1'] = df[df['u_out']==1]['u_in'].groupby(df['breath_id']).cumsum()\n    df['u_in_cumsum_u_out_0'].fillna(0, inplace=True)\n    df['u_in_cumsum_u_out_1'].fillna(0, inplace=True)\n    \n    df['u_in_cumsum_max'] = df.groupby('breath_id')['u_in_cumsum'].transform('max')\n    df['u_in_cumsum_u_out_0_max'] = df.groupby('breath_id')['u_in_cumsum_u_out_0'].transform('max')\n    df['u_in_cumsum_u_out_1_max'] = df.groupby('breath_id')['u_in_cumsum_u_out_1'].transform('max')\n    \n    df['u_in_cumsum_ratio_0'] = (df['u_in_cumsum_u_out_0_max'] / df['u_in_cumsum_max'])#.astype('float32')\n    df['u_in_cumsum_ratio_1'] = (df['u_in_cumsum_u_out_1_max'] / df['u_in_cumsum_max'])#.astype('float32')\n    \n    df = df.drop(['u_in_cumsum_max', 'u_in_cumsum_u_out_0_max', 'u_in_cumsum_u_out_1_max',\n                  'u_in_cumsum_u_out_0', 'u_in_cumsum_u_out_1'], axis=1)\n    print('4 step compelte')\n    #---\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)#.astype('float32')\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)#.astype('float32')\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)#.astype('float32')\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)#.astype('float32')\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)#.astype('float32')\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)#.astype('float32')\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)#.astype('float32')\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)#.astype('float32')\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)#.astype('float32')\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)#.astype('float32')\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)#.astype('float32')\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)#.astype('float32')\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)#.astype('float32')\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)#.astype('float32')\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)#.astype('float32')\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)#.astype('float32')\n    df = df.fillna(0)\n    print('5 step compelte')\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')#.astype('float32')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')#.astype('float32')\n    \n    df['u_in_diff1'] = (df['u_in'] - df['u_in_lag1'])#.astype('float32')\n    df['u_out_diff1'] = (df['u_out'] - df['u_out_lag1'])#.astype('float32')\n    df['u_in_diff2'] = (df['u_in'] - df['u_in_lag2'])#.astype('float32')\n    df['u_out_diff2'] = (df['u_out'] - df['u_out_lag2'])#.astype('float32')\n    \n    df['breath_id__u_in__diffmax'] = (df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in'])#.astype('float32')\n    df['breath_id__u_in__diffmean'] = (df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in'])#.astype('float32')\n    \n    df['u_in_diff3'] = (df['u_in'] - df['u_in_lag3'])#.astype('float32')\n    df['u_out_diff3'] = (df['u_out'] - df['u_out_lag3'])#.astype('float32')\n    df['u_in_diff4'] = (df['u_in'] - df['u_in_lag4'])#.astype('float32')\n    df['u_out_diff4'] = (df['u_out'] - df['u_out_lag4'])#.astype('float32')\n    df['cross']= (df['u_in']*df['u_out'])#.astype('float32')\n    df['cross2']= (df['time_step']*df['u_out'])#.astype('float32')\n    print('6 step compelte')\n\n    # All entries are first point of each breath_id\n    last_df = df.loc[79::80,:]\n    \n    # The Main mode DataFrame and flag\n    main_df= last_df[(last_df['u_in']>4.8)&(last_df['u_in']<5.1)]\n    main_mode_dict = dict(zip(main_df['breath_id'], [1]*len(main_df)))\n    df['main_mode'] = df['breath_id'].map(main_mode_dict) \n    df['main_mode'].fillna(0, inplace=True)\n    df['main_mode'] = df['main_mode']#.astype('int8')\n    del last_df\n    del main_df\n    del main_mode_dict\n    \n    df['u_in_first_label'] = 0\n    df['u_in_last_label'] = 0\n    df['breath_id_u_in_first'] = df.groupby(['breath_id'])['u_in'].transform('first')\n    df['breath_id_u_in_last'] = df.groupby(['breath_id'])['u_in'].transform('last')\n    \n    df.loc[(df['breath_id_u_in_first'] == 0.0), 'u_in_first_label'] = 1\n    df.loc[(df['breath_id_u_in_first'] > 0.0) & (df['breath_id_u_in_first'] < 20), 'u_in_first_label'] = 2\n    df.loc[(df['breath_id_u_in_first'] > 20) & (df['breath_id_u_in_first'] < 50), 'u_in_first_label'] = 3\n    df.loc[(df['breath_id_u_in_first'] > 50) & (df['breath_id_u_in_first'] < 80), 'u_in_first_label'] = 4\n    df.loc[(df['breath_id_u_in_first'] > 80) & (df['breath_id_u_in_first'] < 100), 'u_in_first_label'] = 5\n    df.loc[(df['breath_id_u_in_first'] == 100.0), 'u_in_first_label'] = 6\n    \n    df.loc[(df['breath_id_u_in_last'] >= 0.0) & (df['breath_id_u_in_last'] < 0.75), 'u_in_last_label'] = 1\n    df.loc[(df['breath_id_u_in_last'] >= 0.75) & (df['breath_id_u_in_last'] < 1.75), 'u_in_last_label'] = 2\n    df.loc[(df['breath_id_u_in_last'] >= 1.75) & (df['breath_id_u_in_last'] < 2.1), 'u_in_last_label'] = 3\n    df.loc[(df['breath_id_u_in_last'] >= 2.1) & (df['breath_id_u_in_last'] < 3.0), 'u_in_last_label'] = 4\n    df.loc[(df['breath_id_u_in_last'] >= 3.0) & (df['breath_id_u_in_last'] < 4.8), 'u_in_last_label'] = 5\n    df.loc[(df['breath_id_u_in_last'] >= 4.8) & (df['breath_id_u_in_last'] < 5.1), 'u_in_last_label'] = 6\n    print('7 step compelte')\n    \n    #--add\n    df['u_in_mean'] = df.groupby('breath_id')['u_in'].transform('mean')\n    df['u_in_median'] = df.groupby('breath_id')['u_in'].transform('median')#.astype('float32')\n    df['u_in_min'] = df.groupby('breath_id')['u_in'].transform('min')#.astype('float32')\n    df['u_in_max'] = df.groupby('breath_id')['u_in'].transform('max')#.astype('float32')\n    df['rolling_4_median'] = df.groupby('breath_id')['u_in'].rolling(window=4, min_periods=1).median().reset_index(level=0,drop=True).fillna(0)#.astype('float32')\n    df['rolling_4_max'] = df.groupby('breath_id')['u_in'].rolling(window=4, min_periods=1).max().reset_index(level=0,drop=True).fillna(0)#.astype('float32')\n    df['rolling_4_std'] = df.groupby('breath_id')['u_in'].rolling(window=4, min_periods=1).std().reset_index(level=0,drop=True).fillna(0)#.astype('float32')\n    df['expand_median'] = df.groupby('breath_id')['u_in'].expanding(2).median().reset_index(level=0,drop=True).fillna(0)#.astype('float32')\n    df['expand_max'] = df.groupby('breath_id')['u_in'].expanding(2).max().reset_index(level=0,drop=True).fillna(0)#.astype('float32')\n    df['expand_std'] = df.groupby('breath_id')['u_in'].expanding(2).std().reset_index(level=0,drop=True).fillna(0)#.astype('float32')\n    \n    \"\"\"\n    df['ewm_u_in_mean'] = df.groupby('breath_id')['u_in'].transform(\n        lambda x: x.ewm(halflife=10).mean()\n    ).reset_index(level=0,drop=True).fillna(0)\n    df['ewm_u_in_std'] = df.groupby('breath_id')['u_in'].transform(\n        lambda x: x.ewm(halflife=10).std()\n    ).reset_index(level=0,drop=True).fillna(0)\n    df['ewm_u_in_corr'] = df.groupby('breath_id')['u_in'].transform(\n        lambda x: x.ewm(halflife=10).corr()\n    ).reset_index(level=0,drop=True).fillna(0)\n    \"\"\"\n    #-- \n     \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df['u_in_last_label'] = df['u_in_last_label'].astype(str)\n    df['u_in_first_label'] = df['u_in_first_label'].astype(str)\n\n    df = pd.get_dummies(df)\n    \n    #df['R_C'] = [f'{r:02}_{c:02}' for r, c in zip(df['R'], df['C'])]\n    print('8 step compelte')\n    \n    \"\"\"\n    # new ---\n    # Number of sample points\n    N = 80\n    w = blackman(N+1)\n\n    ffta = lambda x: np.abs(fft(np.append(x.values,x.values[0]))[:80])\n    ffta.__name__ = 'ffta'\n\n    fftw = lambda x: np.abs(fft(np.append(x.values,x.values[0])*w)[:80])\n    fftw.__name__ = 'fftw'\n\n    df['fft_u_in'] = (df.groupby('breath_id')['u_in'].transform(ffta))\n    df['fft_u_in_w'] = df.groupby('breath_id')['u_in'].transform(fftw)\n    df['fft_u_in_w'] = (df['fft_u_in_w'].replace(0,1e-6))\n\n    df['analytical'] = (df.groupby('breath_id')['u_in'].transform(hilbert))\n    df['envelope'] = (np.abs(df['analytical']))\n    df['phase'] = (np.angle(df['analytical']))\n    df['unwrapped_phase'] = (df.groupby('breath_id')['phase'].transform(np.unwrap))\n    df['phase_shift1'] = (df.groupby('breath_id')['unwrapped_phase'].shift(1))\n    df['IF'] = (df['unwrapped_phase'] - df['phase_shift1'])\n    \n    print('9 step compelte')\n    \"\"\"\n    \n    return df\n\ntrain = add_features(train)\ntest = add_features(test)","metadata":{"_uuid":"dc41dbf2-f199-4b9d-bbd9-bf6084162b47","_cell_guid":"13a36b46-7067-4b29-aad3-7e3b15e8415b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-10T02:22:43.674951Z","iopub.execute_input":"2021-11-10T02:22:43.675607Z","iopub.status.idle":"2021-11-10T02:26:18.093431Z","shell.execute_reply.started":"2021-11-10T02:22:43.675571Z","shell.execute_reply":"2021-11-10T02:26:18.092359Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%time\ndef labelencoding(df, column):\n    le = LabelEncoder()\n    le.fit(df[column])\n    return le.transform(df[column])\n\ndf_list = [train, test]\ncolumn_list = ['R', 'C', 'R_C', 'u_in_last_label', 'u_in_first_label']\n\nfor df in df_list:\n    for column in column_list:\n        df[column+'_le'] = labelencoding(df, column)\n        df[column+'_le'] = df[column+'_le'].astype('int8')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## reduce mem.","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                \"\"\"\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                el\n                \"\"\"\n                if (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:18.095464Z","iopub.execute_input":"2021-11-10T02:26:18.095801Z","iopub.status.idle":"2021-11-10T02:26:39.984866Z","shell.execute_reply.started":"2021-11-10T02:26:18.095746Z","shell.execute_reply":"2021-11-10T02:26:39.983883Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Normal","metadata":{}},{"cell_type":"code","source":"%time\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\n#u_outs = train[['u_out']].to_numpy().reshape(-1, 80)\n\n# -- base --\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\n\n#train.drop(['R', 'C', 'R_C', 'u_in_last_label'], axis=1, inplace=True)\n#test = test.drop(['R', 'C', 'R_C', 'u_in_last_label'], axis=1)\n\n#train.drop(['R', 'C', 'R_C', 'u_in_last_label', 'u_in_first_label'], axis=1, inplace=True)\n#test = test.drop(['R', 'C', 'R_C', 'u_in_last_label', 'u_in_first_label'], axis=1)\n# ----------\n\n# -- additional --\n#train.drop(['time_delta', 'time_gap_shift2', 'u_in_first_label_le'], axis=1, inplace=True)\n#test = test.drop(['time_delta', 'time_gap_shift2', 'u_in_first_label_le'], axis=1)\n# ----------------\n\nCOLS = list(train.columns)\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n\nprint('targets shape: ', np.shape(targets))\nprint('train shape: ', np.shape(train))\nprint('test shape: ', np.shape(test))\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:39.986471Z","iopub.execute_input":"2021-11-10T02:26:39.986700Z","iopub.status.idle":"2021-11-10T02:26:55.354661Z","shell.execute_reply.started":"2021-11-10T02:26:39.986673Z","shell.execute_reply":"2021-11-10T02:26:55.353780Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Custom Loss","metadata":{}},{"cell_type":"code","source":"# u_out_index must be set to a correct value!\ndef ventilation_mae_loss(y_true, y_pred, input_tensor, u_out_index=2):\n    w = 1 - tf.expand_dims(input_tensor[:, :, u_out_index], axis=2)\n    mae = w * K.abs(y_true - y_pred)\n    return K.mean(K.sum(mae, axis=-1) / K.sum(w, axis=-1))\n\n\ndef GBVPP_loss(y_true, y_pred, cols = 80):\n    u_out = y_true[:, cols: ]\n    y = y_true[:, :cols ]\n    w = 1 - u_out\n    mae = w * tf.abs(y - y_pred)\n    return tf.reduce_sum(mae, axis=-1) / tf.reduce_sum(w, axis=-1)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(hist):\n    plt.plot(hist.history[\"loss\"])\n    plt.plot(hist.history[\"val_loss\"])\n    plt.title(\"model performance\")\n    plt.ylabel(\"mean_absolute_error\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:55.355971Z","iopub.execute_input":"2021-11-10T02:26:55.356239Z","iopub.status.idle":"2021-11-10T02:26:55.362356Z","shell.execute_reply.started":"2021-11-10T02:26:55.356210Z","shell.execute_reply":"2021-11-10T02:26:55.361372Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Base Model ","metadata":{}},{"cell_type":"code","source":"hidden_base = [1024, 512, 256, 128, 128, 1]\nhidden_ver3 = [800, 600, 400, 200, 50, 1]\n\ndef base_model(input_shape):\n\n                                # train.shape[-2:] = (80, 50)\n    input_1 = keras.layers.Input(shape=input_shape, name='input')\n        \n    LSTM_1 = keras.layers.Bidirectional(\n        keras.layers.LSTM(hidden_ver3[0],\n                          #kernel_initializer='LecunUniform',\n                          #activation='tanh',\n                          return_sequences=True,\n                          dropout=0.0,\n                          stateful=False,\n                          name='LSTM_1'\n                         )\n    )(input_1)\n    \n    LSTM_2 = keras.layers.Bidirectional(\n        keras.layers.LSTM(hidden_ver3[1],\n                          #kernel_initializer='LecunUniform',\n                          #activation='tanh',\n                          return_sequences=True,\n                          dropout=0.1,\n                          stateful=False,\n                          name='LSTM_2'\n                         )\n    )(LSTM_1)\n    \n    LSTM_3 = keras.layers.Bidirectional(\n        keras.layers.LSTM(hidden_ver3[2],\n                          #kernel_initializer='LecunUniform',\n                          #activation='tanh',\n                          return_sequences=True,\n                          dropout=0.1,\n                          stateful=False,\n                          name='LSTM_3'\n                         )\n    )(LSTM_2)\n    \n    LSTM_4 = keras.layers.Bidirectional(\n        keras.layers.LSTM(hidden_ver3[3],\n                          #kernel_initializer='LecunUniform',\n                          #activation='tanh',\n                          return_sequences=True,\n                          dropout=0.0,\n                          stateful=False,\n                          name='LSTM_4'\n                         )\n    )(LSTM_3)\n    \n    Dense_1 = keras.layers.Dense(hidden_ver3[4], \n                                 #kernel_initializer='LecunUniform',\n                                 activation='selu',\n                                 name='Dense_1'\n                                )(LSTM_4)\n    \n    out = keras.layers.Dense(hidden_ver3[5],\n                             activation='linear',\n                             name='out'\n                            )(Dense_1)\n    \n    model = keras.Model(inputs=input_1, outputs=out)\n    \n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## New Model","metadata":{}},{"cell_type":"code","source":"def base_model(input_shape):\n\n                                # train.shape[-2:] = (80, 50)\n    x_input = keras.layers.Input(shape=input_shape, name='input')\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=768, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=512, \n                          dropout=0.1,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=384, \n                          dropout=0.1,\n                          return_sequences=True))(x2)\n    \n    x4 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=256, \n                          dropout=0.1,\n                          return_sequences=True))(x3)\n    \n    x5 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=128, \n                          dropout=0.0,\n                          return_sequences=True))(x4)\n    \n    z2 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=384, \n                         return_sequences=True))(x2)\n    \n    z31 = keras.layers.Multiply()([x3, z2])\n    z31 = keras.layers.BatchNormalization()(z31)\n    z3 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=256, \n                         return_sequences=True))(z31)\n    \n    z41 = keras.layers.Multiply()([x4, z3])\n    z41 = keras.layers.BatchNormalization()(z41)\n    z4 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=128, \n                         return_sequences=True))(z41)\n    \n    z51 = keras.layers.Multiply()([x5, z4])\n    z51 = keras.layers.BatchNormalization()(z51)\n    z5 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=64, \n                         return_sequences=True))(z51)\n    \n    x = keras.layers.Concatenate(axis=2)([x5, z2, z3, z4, z5])\n\n    x = keras.layers.Dense(units=128, activation='selu')(x)\n    \n    x_output = keras.layers.Dense(units=1)(x)\n\n    model = keras.Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def base_model(input_shape):\n\n                                # train.shape[-2:] = (80, 50)\n    x_input = keras.layers.Input(shape=input_shape, name='input')\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=768, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=640, \n                          dropout=0.1,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=512, \n                          dropout=0.1,\n                          return_sequences=True))(x2)\n    \n    x4 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=384, \n                          dropout=0.1,\n                          return_sequences=True))(x3)\n    \n    x5 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=256, \n                          dropout=0.1,\n                          return_sequences=True))(x4)\n    \n    x6 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=128, \n                          dropout=0.0,\n                          return_sequences=True))(x5)\n    \n    z1 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=256, \n                         return_sequences=True))(x3)\n    \n    z21 = keras.layers.Multiply()([x5, z1])\n    z21 = keras.layers.BatchNormalization()(z21)\n    z3 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=128, \n                         return_sequences=True))(z21)\n    \n    z31 = keras.layers.Multiply()([x6, z3])\n    z31 = keras.layers.BatchNormalization()(z31)\n    z4 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=64, \n                         return_sequences=True))(z31)\n    \n    x = keras.layers.Concatenate(axis=2)([x6, z4])\n    \n    x = keras.layers.Dense(units=128, activation='selu')(x)\n    \n    x_output = keras.layers.Dense(units=1)(x)\n\n    model = keras.Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def base_model(input_shape):\n\n                                # train.shape[-2:] = (80, 50)\n    x_input = keras.layers.Input(shape=input_shape, name='input')\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=448, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=320, \n                          dropout=0.1,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=192, \n                          dropout=0.1,\n                          return_sequences=True))(x2)\n    \n    x4 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=64, \n                          dropout=0.0,\n                          return_sequences=True))(x3)\n\n    x = keras.layers.Dense(units=128, activation='selu')(x41)\n    \n    x_output = keras.layers.Dense(units=1)(x)\n\n    model = keras.Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_unit = [448, 320, 192, 64]\nadd_unit = 0\n\ndef LSTM_block(x_input):\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit[0]+add_unit, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit[1]+add_unit, \n                          dropout=0.2,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit[2]+add_unit, \n                          dropout=0.1,\n                          return_sequences=True))(x2)\n    \n    x4 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit[3]+add_unit, \n                          dropout=0.0,\n                          return_sequences=True))(x3)\n    \n    return x4\n\n\ndef GRU_block(x_input):\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit[0]+add_unit, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit[1]+add_unit, \n                          dropout=0.2,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit[2]+add_unit, \n                          dropout=0.1,\n                          return_sequences=True))(x2)\n    \n    x4 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit[3]+add_unit, \n                          dropout=0.0,\n                          return_sequences=True))(x3)\n    \n    return x4","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:55.363417Z","iopub.execute_input":"2021-11-10T02:26:55.363653Z","iopub.status.idle":"2021-11-10T02:26:55.377023Z","shell.execute_reply.started":"2021-11-10T02:26:55.363627Z","shell.execute_reply":"2021-11-10T02:26:55.376259Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"layer_unit2 = [320, 192, 64]\nadd_unit2 = 0\n\ndef LSTM_block2(x_input):\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit2[0]+add_unit2, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit2[1]+add_unit2, \n                          dropout=0.1,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=layer_unit2[2]+add_unit2, \n                          dropout=0.0,\n                          return_sequences=True))(x2)\n    \n    return x3\n\n\ndef GRU_block2(x_input):\n    \n    x1 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit2[0]+add_unit2, \n                          dropout=0.0,\n                          return_sequences=True))(x_input)\n    \n    x2 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit2[1]+add_unit2, \n                          dropout=0.1,\n                          return_sequences=True))(x1)\n    \n    x3 = keras.layers.Bidirectional(\n        keras.layers.GRU(units=layer_unit2[2]+add_unit2, \n                          dropout=0.0,\n                          return_sequences=True))(x2)\n    \n    return x3","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:55.378124Z","iopub.execute_input":"2021-11-10T02:26:55.378877Z","iopub.status.idle":"2021-11-10T02:26:55.396041Z","shell.execute_reply.started":"2021-11-10T02:26:55.378827Z","shell.execute_reply":"2021-11-10T02:26:55.395272Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def base_model(input_shape):\n\n                                # train.shape[-2:] = (80, 50)\n    x_input = keras.layers.Input(shape=input_shape, name='input')\n    \n    block_output_1 = LSTM_block(x_input)\n    block_output_2 = GRU_block(x_input)\n    \n    mul1 = keras.layers.Multiply()([block_output_1, block_output_2])\n    add1 = keras.layers.Add()([block_output_1, block_output_2])\n    concat_1 = keras.layers.Concatenate(axis=2)([mul1, add1])\n    concat_1 = keras.layers.BatchNormalization()(concat_1)\n    \n    block_output_3 = LSTM_block2(concat_1)\n    block_output_4 = GRU_block2(concat_1)\n    \n    mul2 = keras.layers.Multiply()([block_output_3, block_output_4])\n    add2 = keras.layers.Add()([block_output_3, block_output_4])\n    concat_2 = keras.layers.Concatenate(axis=2)([mul2, add2])\n    concat_2 = keras.layers.BatchNormalization()(concat_2)\n    \n    x = keras.layers.Bidirectional(\n        keras.layers.LSTM(units=128, \n                          dropout=0.0,\n                          return_sequences=True))(concat_2)\n    \n    x = keras.layers.Dense(units=128, activation='selu')(x)\n    \n    x_output = keras.layers.Dense(units=1)(x)\n\n    model = keras.Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:55.397445Z","iopub.execute_input":"2021-11-10T02:26:55.397905Z","iopub.status.idle":"2021-11-10T02:26:55.416435Z","shell.execute_reply.started":"2021-11-10T02:26:55.397872Z","shell.execute_reply":"2021-11-10T02:26:55.415361Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = base_model(train.shape[-2:])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:26:55.418514Z","iopub.execute_input":"2021-11-10T02:26:55.418898Z","iopub.status.idle":"2021-11-10T02:27:03.609351Z","shell.execute_reply.started":"2021-11-10T02:26:55.418850Z","shell.execute_reply":"2021-11-10T02:27:03.608114Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## TPU","metadata":{}},{"cell_type":"code","source":"%%time\n\nEPOCH=200\nBATCH_SIZE=1024\nNUM_FOLDS=10\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    test_preds = []\n    history_loss = [] \n    history_val_loss = []\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        #u_out_train, u_out_valid = u_outs[train_idx], u_outs[test_idx]    \n        \n        U_OUT_IDX = 2\n        y_weight = np.ones_like(y_train)\n        u_out_values = X_train[:, :, U_OUT_IDX]\n        y_weight[u_out_values==0] = 0\n    \n        model = base_model(train.shape[-2:])\n        \n        #model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=GBVPP_loss)\n        #model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mae')\n        model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mae',\n                  sample_weight_mode=\"temporal\")\n        \n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n        es = EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1, mode=\"min\", restore_best_weights=True)\n    \n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n        )\n        \n        history = model.fit(\n            #X_train, np.append(y_train, u_out_train, axis=1), \n            X_train, y_train, \n            #validation_data=(X_valid, np.append(y_valid, u_out_valid, axis=1)),\n            validation_data=(X_valid, y_valid),\n            sample_weight=y_weight.reshape((-1, 80, 1)),\n            epochs=EPOCH, \n            batch_size=BATCH_SIZE, \n            callbacks=[lr, es, sv]\n        ) \n        \n        history_loss.append(history.history['loss'])\n        history_val_loss.append(history.history['val_loss'])\n        \n        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n        \n        if ONE_FOLD_PLOT:\n            plot_hist(history)\n        \n        if COMPUTE_LSTM_IMPORTANCE:\n            results = []\n            print(' Computing LSTM feature importance...')\n            \n            # COMPUTE BASELINE (NO SHUFFLE)\n            oof_preds = model.predict(X_valid, verbose=0).squeeze()\n            baseline_mae = np.mean(np.abs( oof_preds-y_valid ))\n            results.append({'feature':'BASELINE','mae':baseline_mae})           \n\n            for k in tqdm(range(len(COLS))):\n                \n                # SHUFFLE FEATURE K\n                save_col = X_valid[:,:,k].copy()\n                np.random.shuffle(X_valid[:,:,k])\n                        \n                # COMPUTE OOF MAE WITH FEATURE K SHUFFLED\n                oof_preds = model.predict(X_valid, verbose=0).squeeze() \n                mae = np.mean(np.abs( oof_preds-y_valid ))\n                results.append({'feature':COLS[k],'mae':mae})\n                X_valid[:,:,k] = save_col\n         \n            # DISPLAY LSTM FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('mae')\n            plt.figure(figsize=(10,20))\n            plt.barh(np.arange(len(COLS)+1),df.mae)\n            plt.yticks(np.arange(len(COLS)+1),df.feature.values)\n            plt.title('LSTM Feature Importance',size=16)\n            plt.ylim((-1,len(COLS)+1))\n            plt.plot([baseline_mae,baseline_mae],[-1,len(COLS)+1], '--', color='orange',\n                     label=f'Baseline OOF\\nMAE={baseline_mae:.3f}')\n            plt.xlabel(f'Fold {fold+1} OOF MAE with feature permuted',size=14)\n            plt.ylabel('Feature',size=14)\n            plt.legend()\n            plt.show()\n                               \n            # SAVE LSTM FEATURE IMPORTANCE\n            df = df.sort_values('mae',ascending=False)\n            df.to_csv(f'lstm_feature_importance_fold_{fold+1}.csv',index=False)\n            \n        del X_train, X_valid, y_train, y_valid, model#, u_out_train, u_out_valid\n        gc.collect()\n        \n        # ONLY DO ONE FOLD\n        if ONE_FOLD_ONLY: break","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:27:03.612835Z","iopub.execute_input":"2021-11-10T02:27:03.613126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"postfix = '_fold_10'\nif ONE_FOLD_ONLY:\n    NUM_FOLDS = 1\n    postfix = '_fold_1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss = []\nfor idx in range(NUM_FOLDS):\n    val_loss.append(np.min(history_val_loss[idx]))\n    print(np.min(history_val_loss[idx]))\n\nprint('----------------------------------')\ncv_score = np.round(np.mean(val_loss), 4)\nprint('10 folds mean cv score is ', cv_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ENSEMBLE FOLDS WITH MEAN\nsubmission[\"pressure\"] = sum(test_preds)/NUM_FOLDS\nsubmission.to_csv('submission_mean_cv:'+str(cv_score)+postfix+'.csv', index=False)\n\n# ENSEMBLE FOLDS WITH MEDIAN\nsubmission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\nsubmission.to_csv('submission_median_cv:'+str(cv_score)+postfix+'.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\nsubmission[\"pressure\"] =\\\n    np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\nsubmission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\nsubmission.to_csv('submission_median_round_cv:'+str(cv_score)+postfix+'.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'../working')\nfrom IPython.display import FileLink\nFileLink(r'folds9.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}